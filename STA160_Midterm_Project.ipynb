{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tspacel/gh-usernames/blob/master/STA160_Midterm_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0mHB6rEwC0b"
      },
      "source": [
        "# STA 160 Midterm Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OACHQrdU6_HG"
      },
      "source": [
        "## 1.1 Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4ngLM3azIK4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7Z9orh4zVkl"
      },
      "outputs": [],
      "source": [
        "\n",
        "#load data from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjHyJaze0RT7"
      },
      "outputs": [],
      "source": [
        "#drive.mount('/content/gdrive')\n",
        "path = '/gdrive/MyDrive/heart_disease_health_indicators_BRFSS2015.csv'\n",
        "heart_disease = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1t4TSbF7Hz5"
      },
      "source": [
        "## 1.2 Overview of the data structure and outline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0LDMiWg6kkp"
      },
      "outputs": [],
      "source": [
        "print (heart_disease.head(10))\n",
        "print( heart_disease.describe())\n",
        "print(heart_disease.info())\n",
        "print(heart_disease.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFZDX2Ne6748"
      },
      "source": [
        "From the out put above we can see that the dataset contains 253680 rows of data and 21 different columns , each rows are make up by the non-null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = heart_disease['HeartDiseaseorAttack']\n",
        "X = heart_disease[[ 'HighBP', 'HighChol', 'CholCheck', 'BMI',\n",
        "       'Smoker', 'Stroke', 'Diabetes', 'PhysActivity', 'Fruits', 'Veggies',\n",
        "       'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth',\n",
        "       'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education',\n",
        "       'Income']]\n",
        "\n",
        "# Calculate entropy of target variable\n",
        "def entropy(y):\n",
        "    counts = y.value_counts()\n",
        "    probabilities = counts / counts.sum()\n",
        "    return -sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "entropy_y = entropy(y)\n",
        "\n",
        "# Calculate information gain of each feature\n",
        "information_gains = []\n",
        "for feature in X:\n",
        "    # Calculate entropy of feature given target variable\n",
        "    unique_values = X[feature].unique()\n",
        "    entropy_feature_given_y = 0\n",
        "    for value in unique_values:\n",
        "        mask = X[feature] == value\n",
        "        y_given_value = y[mask]\n",
        "        entropy_feature_given_y += mask.sum() / len(X) * entropy(y_given_value)\n",
        "\n",
        "    # Calculate information gain\n",
        "    information_gain = entropy_y - entropy_feature_given_y\n",
        "    information_gains.append(information_gain)\n",
        "\n",
        "# Rank features by information gain (high to low)\n",
        "feature_ranks = np.argsort(information_gains)[::-1]\n",
        "feature_names = X.columns[feature_ranks]\n",
        "\n",
        "# Print results\n",
        "for rank, feature_name in enumerate(feature_names):\n",
        "    print(f\"{rank+1}. {feature_name}: {information_gains[feature_ranks[rank]]:.4f}\")"
      ],
      "metadata": {
        "id": "MWH5aghUZEVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "I(X; Y) = \\sum_{y \\in Y} \\sum_{x \\in X} p(x, y) \\log \\frac{p(x, y)}{p(x) p(y)}\n",
        "\\end{equation}"
      ],
      "metadata": {
        "id": "ee5fKTjwMYo8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJz2MjGpezfo"
      },
      "source": [
        "### Dimionation reduction to creat the 2x2 dim contingency table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRY6VYgt61Lu"
      },
      "outputs": [],
      "source": [
        "data_crosstab = pd.crosstab(heart_disease['HighBP'],\n",
        "                            heart_disease['HeartDiseaseorAttack'], \n",
        "                            margins = False)\n",
        "print(data_crosstab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHriZxb2e_aX"
      },
      "outputs": [],
      "source": [
        "data_crosstab = pd.crosstab([heart_disease.HighBP, heart_disease.HighChol], \n",
        "                             heart_disease.HeartDiseaseorAttack, margins = False)\n",
        "print(data_crosstab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn3DhkSBh96k"
      },
      "outputs": [],
      "source": [
        "data_crosstab = pd.crosstab([heart_disease.HighBP, heart_disease.HighChol,heart_disease.Age], \n",
        "                             heart_disease.HeartDiseaseorAttack, margins = False)\n",
        "print(data_crosstab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCc0aGkWjy0h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the feature columns and target column\n",
        "features = ['HighBP', 'HighChol', 'Age']\n",
        "target = 'HeartDiseaseorAttack'\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(heart_disease[features], \n",
        "                                                    heart_disease[target], \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=42)\n",
        "\n",
        "# Initialize the decision tree classifier with a maximum depth of 3\n",
        "clf = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "# Train the classifier on the training set\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Plot the decision tree\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_tree(clf, feature_names=features, class_names=['No Disease', 'Disease'], filled=True)\n",
        "plt.show()\n",
        "\n",
        "# Use the trained classifier to make predictions on the testing set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Print the accuracy score of the classifier\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjGNSsYYk1fG"
      },
      "source": [
        "In this program, we first load the 'heart_disease' dataset using pandas and define the feature columns ('HighBP', 'HighChol', and 'Age') and the target column ('HeartDiseaseorAttack'). We then split the data into training and testing sets using the train_test_split function from scikit-learn.\n",
        "\n",
        "Next, we initialize a decision tree classifier with a maximum depth of 3 and train it on the training set using the fit method. Finally, we plot the resulting decision tree using the plot_tree function from scikit-learn, which takes as input the trained classifier (clf), the feature names (features), the class names ('No Disease' and 'Disease'), and the filled argument, which fills the tree nodes with colors according to the class distribution. We also set the figsize argument to (10, 6) to make the plot larger."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = heart_disease['HeartDiseaseorAttack']\n",
        "X = heart_disease[[ 'HighBP', 'HighChol', 'CholCheck', 'BMI',\n",
        "       'Smoker', 'Stroke', 'Diabetes', 'PhysActivity', 'Fruits', 'Veggies',\n",
        "       'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth',\n",
        "       'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education',\n",
        "       'Income']]\n",
        "def entropy(y):\n",
        "    counts = y.value_counts()\n",
        "    probabilities = counts / counts.sum()\n",
        "    return -sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "entropy_y = entropy(y)\n",
        "\n",
        "# Calculate information gain of each feature\n",
        "information_gains = []\n",
        "for feature in X:\n",
        "    # Calculate entropy of feature given target variable\n",
        "    unique_values = X[feature].unique()\n",
        "    entropy_feature_given_y = 0\n",
        "    for value in unique_values:\n",
        "        mask = X[feature] == value\n",
        "        y_given_value = y[mask]\n",
        "        entropy_feature_given_y += mask.sum() / len(X) * entropy(y_given_value)\n",
        "\n",
        "    # Calculate information gain\n",
        "    information_gain = entropy_y - entropy_feature_given_y\n",
        "    information_gains.append(information_gain)\n",
        "\n",
        "# Rank features by information gain (high to low)\n",
        "feature_ranks = np.argsort(information_gains)[::-1]\n",
        "feature_names = X.columns[feature_ranks]\n",
        "information_gains_sorted = [information_gains[i] for i in feature_ranks]\n",
        "\n",
        "# Plot horizontal bar chart of information gain for each feature\n",
        "plt.barh(feature_names, information_gains_sorted)\n",
        "plt.title(\"Information Gain of Features in Heart Disease Dataset\")\n",
        "plt.xlabel(\"Information Gain\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rBEakaYNi5Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo_estimate(feature, n_samples=1000):\n",
        "    y_given_feature = []\n",
        "    unique_values = heart_disease[feature].unique()\n",
        "    probabilities = heart_disease[feature].value_counts(normalize=True)\n",
        "    for _ in range(n_samples):\n",
        "        value = np.random.choice(unique_values, p=probabilities)\n",
        "        y_given_value = heart_disease.loc[heart_disease[feature] == value, 'HeartDiseaseorAttack']\n",
        "        y_given_feature.append(np.random.choice(y_given_value))\n",
        "    return y_given_feature\n",
        "\n",
        "# Plot histograms of target variable given each feature using Monte Carlo estimates\n",
        "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\n",
        "for feature, ax in zip(['HighBP', 'HighChol', 'Age', 'CholCheck', 'BMI', 'Smoker', 'Diabetes', 'PhysActivity', 'Fruits'], axs.flatten()):\n",
        "    y_given_feature = monte_carlo_estimate(feature)\n",
        "    ax.hist(y_given_feature, bins=[0, 1, 2], align='left')\n",
        "    ax.set_title(f\"HeartDiseaseorAttack given {feature}\")\n",
        "    ax.set_xlabel(\"HeartDiseaseorAttack\")\n",
        "    ax.set_ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "blCWMnCoj7Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "# Drop the target column (we don't want to include it in the PCA)\n",
        "target_col = 'HeartDiseaseorAttack'\n",
        "X = heart_disease.drop(target_col, axis=1)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_std = scaler.fit_transform(X)\n",
        "\n",
        "# Create a PCA object with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit the PCA model to the data\n",
        "pca.fit(X_std)\n",
        "\n",
        "# Transform the data into the new feature space\n",
        "X_pca = pca.transform(X_std)\n",
        "\n",
        "# Plot the results\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()\n",
        "\n",
        "# Fit the PCA model to the data\n",
        "pca.fit(X_std)\n",
        "\n",
        "# Print the explained variance of the top 2 components\n",
        "print('Explained variance of top 2 components:', pca.explained_variance_ratio_[:2])\n"
      ],
      "metadata": {
        "id": "62RLPyOZl93T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(heart_disease.corr(), cmap='coolwarm', center=0)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Calculate the absolute correlation between 'HeartDiseaseorAttack' and the other columns\n",
        "corr = heart_disease.corr()['HeartDiseaseorAttack'].drop('HeartDiseaseorAttack').abs().sort_values(ascending=False)\n",
        "\n",
        "# Print the correlation values in descending order\n",
        "print(corr)"
      ],
      "metadata": {
        "id": "ZdXLREzKmzNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oggMtF6ln2_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BxDz-HTkzS1"
      },
      "outputs": [],
      "source": [
        "# Bin the data by age\n",
        "age_bins = heart_disease['Age']\n",
        "\n",
        "# Calculate the conditional probability of HeartDiseaseorAttack for each combination of HighBP, HighChol, and Age\n",
        "heart_disease_cond_prob = heart_disease.groupby(['HighBP', 'HighChol', 'Age'])['HeartDiseaseorAttack'].mean().reset_index()\n",
        "\n",
        "# Plot the conditional probability as a histogram for each combination of HighBP and HighChol\n",
        "for high_bp in [0, 1]:\n",
        "    for high_chol in [0, 1]:\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        plt.hist(heart_disease_cond_prob[(heart_disease_cond_prob['HighBP'] == high_bp) & (heart_disease_cond_prob['HighChol'] == high_chol)]['HeartDiseaseorAttack'], bins=20, alpha=0.5, label='Heart Disease')\n",
        "        plt.xlabel('Conditional Probability of Heart Disease')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('Conditional Probability of Heart Disease Given HighBP={} and HighChol={}'.format(high_bp, high_chol))\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsEXhmxpl4ql"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import entropy\n",
        "\n",
        "# Calculate the conditional entropy of HeartDiseaseorAttack for each combination of HighBP, HighChol, and Age\n",
        "heart_disease_cond_entropy = heart_disease.groupby(['HighBP', 'HighChol', 'Age'])['HeartDiseaseorAttack'].apply(lambda x: entropy(np.histogram(x, bins=[0, 0.5, 1])[0], base=2)).reset_index()\n",
        "heart_disease_cond_entropy = heart_disease_cond_entropy.rename(columns={'HeartDiseaseorAttack': 'Entropy'})\n",
        "age = heart_disease.Age\n",
        "# Plot the conditional entropy as a heatmap for each combination of HighBP and HighChol\n",
        "heatmap_data = heart_disease_cond_entropy.pivot(index=['Age', 'HighBP'], columns='HighChol', values='Entropy')\n",
        "plt.imshow(heatmap_data, cmap='YlOrRd', interpolation='nearest')\n",
        "plt.colorbar()\n",
        "plt.xticks(np.arange(len([0, 1])), [0, 1])\n",
        "#plt.yticks(age)\n",
        "plt.xlabel('HighChol')\n",
        "plt.ylabel('Age')\n",
        "plt.title('Conditional Entropy of Heart Disease Given HighBP and HighChol')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I7tmIzWkPZUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "H(Y|X) = -\\sum_{x \\in X} \\sum_{y \\in Y} p(x,y) \\log\\frac{p(x)}{p(x,y)}\n",
        "\\end{equation}\n",
        "\n",
        "where $p(x,y)$ is the joint probability distribution of X and Y, and $p(x)$ is the marginal probability distribution of X. This formula measures the amount of uncertainty in Y given X."
      ],
      "metadata": {
        "id": "6toc4BVqPaGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## "
      ],
      "metadata": {
        "id": "9Xw4-ml4xxKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define feature columns and target column\n",
        "feature_cols = ['HighBP', 'HighChol', 'Age']\n",
        "target_col = 'HeartDiseaseorAttack'\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(heart_disease[feature_cols], heart_disease[target_col], test_size=0.2, random_state=42)# Split the data into training and testing sets\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(len(feature_cols),)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_data, train_labels, epochs=1, batch_size=32, validation_data=(test_data, test_labels))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "DChfpO6ww0jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation accuracy over epochs\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation loss over epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l3BDKKvx831o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "# Define feature columns and target column\n",
        "\n",
        "# Prepare the data\n",
        "X = heart_disease[['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', 'Diabetes', \n",
        "        'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'AnyHealthcare', \n",
        "        'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', \n",
        "        'Education', 'Income']].values\n",
        "y = heart_disease['HeartDiseaseorAttack'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "8yResb4A2AJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "24B5wlJn8wpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this program, we first load the 'heart_disease' dataset using pandas and define the feature columns ('HighBP', 'HighChol', and 'Age') and the target column ('HeartDiseaseorAttack'). We then split the data into training and testing sets using the train_test_split function from scikit-learn.\n",
        "\n",
        "Next, we define the DNN using TensorFlow's Sequential API. The model has three layers: the first two are fully connected (Dense) layers with 64 and 32 neurons, respectively, and ReLU activation functions; the third layer is a fully connected layer with one neuron and a sigmoid activation function, which outputs a probability of the positive class (disease).\n",
        "\n",
        "We compile the model by specifying the optimizer (adam), loss function (binary_crossentropy), and metric to optimize (accuracy).\n",
        "\n",
        "We train the model on the training data using the fit method, with 50 epochs and a batch size of 32. We also pass in the testing data to evaluate the model's performance on unseen data."
      ],
      "metadata": {
        "id": "v8ODjBmkxs1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plot the training and validation accuracy over epochs\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation loss over epochs\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mB6_Hwa-yftg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a dataframe for HighBP\n",
        "high_bp_df = heart_disease[heart_disease['HighBP'] == 1]\n",
        "\n",
        "# Create a dataframe for normal BP\n",
        "normal_bp_df = heart_disease[heart_disease['HighBP'] == 0]\n",
        "\n",
        "# Create a dataframe for HighChol\n",
        "high_chol_df = heart_disease[heart_disease['HighChol'] == 1]\n",
        "\n",
        "# Create a dataframe for normal Chol\n",
        "normal_chol_df = heart_disease[heart_disease['HighChol'] == 0]\n"
      ],
      "metadata": {
        "id": "IH0u0_gM96T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Define the feature columns and the target column\n",
        "features = ['Age']\n",
        "target = 'HeartDiseaseorAttack'\n",
        "\n",
        "# Compute the conditional entropy of the target column based on the feature columns\n",
        "entropy = {}\n",
        "for col in features:\n",
        "    for val in set(high_bp_df[col]):\n",
        "        subdata = high_bp_df[high_bp_df[col] == val]\n",
        "        p = subdata[target].value_counts(normalize=True)\n",
        "        entropy[(col, val)] = -np.sum(p * np.log2(p))\n",
        "\n",
        "# Plot the conditional entropy as a line chart\n",
        "x = np.arange(len(entropy))\n",
        "y = list(entropy.values())\n",
        "labels = ['{}={}'.format(*k) for k in entropy.keys()]\n",
        "plt.plot(x, y)\n",
        "plt.xticks(x, labels, rotation='vertical')\n",
        "plt.xlabel('Condition')\n",
        "plt.ylabel('Entropy')\n",
        "plt.title('Conditional Entropy of HeartDiseaseorAttack')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kAZyeHYPzOKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the feature columns and the target column\n",
        "features = ['Age']\n",
        "target = 'HeartDiseaseorAttack'\n",
        "\n",
        "# Compute the conditional entropy of the target column based on the feature columns\n",
        "entropy = {}\n",
        "for col in features:\n",
        "    for val in set(normal_bp_df[col]):\n",
        "        subdata = normal_bp_df[normal_bp_df[col] == val]\n",
        "        p = subdata[target].value_counts(normalize=True)\n",
        "        entropy[(col, val)] = -np.sum(p * np.log2(p))\n",
        "\n",
        "# Plot the conditional entropy as a line chart\n",
        "x = np.arange(len(entropy))\n",
        "y2 = list(entropy.values())\n",
        "labels = ['{}={}'.format(*k) for k in entropy.keys()]\n",
        "plt.plot(x, y, y2)\n",
        "plt.xticks(x, labels, rotation='vertical')\n",
        "plt.xlabel('Condition')\n",
        "plt.ylabel('Entropy')\n",
        "plt.title('Conditional Entropy of HeartDiseaseorAttack')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EaXv_-Od-XQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the feature columns and the target column\n",
        "features = ['Age']\n",
        "target = 'HeartDiseaseorAttack'\n",
        "\n",
        "# Compute the conditional entropy of the target column based on the feature columns\n",
        "entropy = {}\n",
        "for col in features:\n",
        "    for val in set(high_chol_df[col]):\n",
        "        subdata = high_chol_df[high_chol_df[col] == val]\n",
        "        p = subdata[target].value_counts(normalize=True)\n",
        "        entropy[(col, val)] = -np.sum(p * np.log2(p))\n",
        "\n",
        "# Plot the conditional entropy as a line chart\n",
        "x = np.arange(len(entropy))\n",
        "y3 = list(entropy.values())\n",
        "entropy = {}\n",
        "for col in features:\n",
        "    for val in set(normal_chol_df[col]):\n",
        "        subdata = normal_chol_df[normal_chol_df[col] == val]\n",
        "        p = subdata[target].value_counts(normalize=True)\n",
        "        entropy[(col, val)] = -np.sum(p * np.log2(p))\n",
        "\n",
        "# Plot the conditional entropy as a line chart\n",
        "x = np.arange(len(entropy))\n",
        "y4 = list(entropy.values())\n",
        "plt.plot(x ,y3)\n",
        "plt.xticks(x, labels, rotation='vertical')\n",
        "plt.xlabel('Condition')\n",
        "plt.ylabel('Entropy')\n",
        "plt.title('Conditional Entropy of HeartDiseaseorAttack')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bMr8rqRfa1ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the variables of interest\n",
        "variables = ['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Diabetes', 'PhysActivity', 'Fruits']\n",
        "\n",
        "# Define the age groups\n",
        "age_groups = [('1', heart_disease.loc[heart_disease['Age']==1]), \n",
        "              ('2', heart_disease.loc[heart_disease['Age']==2]),\n",
        "              ('3', heart_disease.loc[heart_disease['Age']==3]),\n",
        "              ('4', heart_disease.loc[heart_disease['Age']==4]),\n",
        "              ('5', heart_disease.loc[heart_disease['Age']==5]),\n",
        "              ('6', heart_disease.loc[heart_disease['Age']==6]),\n",
        "              ('7', heart_disease.loc[heart_disease['Age']==7]),\n",
        "              ('8', heart_disease.loc[heart_disease['Age']==8]),\n",
        "              ('9', heart_disease.loc[heart_disease['Age']==9]),\n",
        "              ('10', heart_disease.loc[heart_disease['Age']==10]),\n",
        "              ('11', heart_disease.loc[heart_disease['Age']==11]),\n",
        "              ('12', heart_disease.loc[heart_disease['Age']==12])]\n",
        "\n",
        "# Loop over the age groups and variables\n",
        "for age_group in age_groups:\n",
        "    age_label = age_group[0]\n",
        "    age_data = age_group[1]\n",
        "    for var in variables:\n",
        "        # Create a contingency table\n",
        "        contingency_table = pd.crosstab(index=age_data[var], columns=[age_data['Age'], age_data['HeartDiseaseorAttack']], margins=True)\n",
        "        \n",
        "        # Compute the marginal probabilities\n",
        "        P_age = contingency_table.iloc[:-1,:-1].sum(axis=1) / contingency_table.iloc[:-1,:-1].sum().sum()\n",
        "        P_hd = contingency_table.iloc[:-1,:-1].sum(axis=0) / contingency_table.iloc[:-1,:-1].sum().sum()\n",
        "        P_var = contingency_table.iloc[:-1,:-1].sum(axis=0, level=0) / contingency_table.iloc[:-1,:-1].sum().sum()\n",
        "        \n",
        "        # Compute the joint probabilities\n",
        "        P_age_hd_var = contingency_table.iloc[:-1,:-1] / contingency_table.iloc[:-1,:-1].sum().sum()\n",
        "        \n",
        "        # Compute the conditional mutual information\n",
        "        MI_cond = 0\n",
        "        for a in np.unique(age_data['Age']):\n",
        "            for h in np.unique(age_data['HeartDiseaseorAttack']):\n",
        "                P_var_given_ah = P_age_hd_var.loc[:,(a,h)] / P_hd[h] / P_age[a]\n",
        "                P_var_given_a = P_var.loc[:,a] / P_age[a]\n",
        "                MI_cond += P_age_hd_var.loc[:,(a,h)] * np.log2(P_var_given_ah / P_var_given_a)\n",
        "        MI_cond = MI_cond.sum()\n",
        "        \n",
        "        # Plot the conditional mutual information\n",
        "        plt.scatter(age_label, MI_cond, label=var)\n",
        "    plt.title('Conditional Mutual Information')"
      ],
      "metadata": {
        "id": "6C5q0sAtEY17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(heart_disease.drop('HeartDiseaseorAttack', axis=1), \n",
        "                                                    heart_disease['HeartDiseaseorAttack'], \n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=42)\n",
        "\n",
        "# Initialize the random forest model with 100 trees\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Print the accuracy score of the model on the test set\n",
        "print('Accuracy:', rf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "BPsS4xa28NaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "# Select features and target\n",
        "features = ['HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker', 'Stroke', \n",
        "            'Diabetes', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', \n",
        "            'AnyHealthcare', 'NoDocbcCost', 'GenHlth', 'MentHlth', 'PhysHlth', \n",
        "            'DiffWalk', 'Sex', 'Age', 'Education', 'Income']\n",
        "target = 'HeartDiseaseorAttack'\n",
        "\n",
        "X = heart_disease[features]\n",
        "y = heart_disease[target]\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = rfc.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "h5bkRrlX9hgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "amPID5-eRsaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "\\hat{y} = \\frac{1}{B} \\sum_{i=1}^B f_i(x)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\hat{y}$ is the predicted output, $B$ is the number of decision trees in the forest, and $f_i(x)$ is the prediction made by the $i$-th decision tree based on the input vector $x$. In addition, random forest uses bootstrap aggregating (bagging) and random feature selection to build multiple decision trees and reduce overfitting.\n",
        "\n",
        "Each decision tree in the random forest is trained on a random subset of the training data, and a random subset of the features is selected for each split. This randomness helps to reduce the correlation between the trees and improve the generalization performance of the model."
      ],
      "metadata": {
        "id": "qWOT6_EVRs-w"
      }
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "history_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}